{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0TgoxS2iuB0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from collections import Counter\n",
        "import re\n",
        "import operator\n",
        "import nltk\n",
        "import heapq\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "import gensim.downloader as api"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deiq4H44iyds"
      },
      "source": [
        "def text_to_bow(text):\n",
        "    \"\"\" convert text string to an array of token counts. Use bow_vocabulary. \"\"\"\n",
        "    #<YOUR CODE>\n",
        "    sentence_vectors = []\n",
        "    sentence_tokens = tokenizer.tokenize(text)\n",
        "    for token in bow_vocabulary:\n",
        "        if token in sentence_tokens:\n",
        "            sentence_vectors.append(sentence_tokens.count(token))\n",
        "        else:\n",
        "            sentence_vectors.append(0)\n",
        "        #sentence_vectors.append(sent_vec)\n",
        "    sentence_vectors = np.asarray(sentence_vectors)\n",
        "    return sentence_vectors"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSPGKMqci_e4"
      },
      "source": [
        "def create_model(input_size, lr=0.1):\n",
        "  model = nn.Sequential()\n",
        "  model.add_module('l1', nn.Linear(input_size,2))\n",
        "\n",
        "  opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  return model, opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCuHoYHXjzX7"
      },
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    opt,\n",
        "    lr_scheduler,\n",
        "    X_train_torch,\n",
        "    y_train_torch,\n",
        "    X_val_torch,\n",
        "    y_val_torch,\n",
        "    n_iterations=500,\n",
        "    batch_size=32,\n",
        "    warm_start=False,\n",
        "    show_plots=True,\n",
        "    eval_every=10\n",
        "):\n",
        "    if not warm_start:\n",
        "        for name, module in model.named_children():\n",
        "            print('resetting ', name)\n",
        "            try:\n",
        "                module.reset_parameters()\n",
        "            except AttributeError as e:\n",
        "                print('Cannot reset {} module parameters: {}'.format(name, e))\n",
        "\n",
        "    train_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    local_train_loss_history = []\n",
        "    local_train_acc_history = []\n",
        "    for i in range(n_iterations):\n",
        "\n",
        "        # sample 256 random observations\n",
        "        ix = np.random.randint(0, len(X_train_torch), batch_size)\n",
        "        x_batch = X_train_torch[ix]\n",
        "        y_batch = y_train_torch[ix]\n",
        "\n",
        "        # predict log-probabilities or logits\n",
        "        y_predicted = model(x_batch) ### YOUR CODE\n",
        "\n",
        "        # compute loss, just like before\n",
        "        ### YOUR CODE\n",
        "        loss = loss_function(y_predicted, y_batch)\n",
        "\n",
        "        # compute gradients\n",
        "        ### YOUR CODE\n",
        "        loss.backward()\n",
        "\n",
        "        # Adam step\n",
        "        ### YOUR CODE\n",
        "        opt.step()\n",
        "        # clear gradients\n",
        "        ### YOUR CODE\n",
        "        opt.zero_grad()\n",
        "\n",
        "        local_train_loss_history.append(loss.data.numpy())\n",
        "        local_train_acc_history.append(\n",
        "            accuracy_score(\n",
        "                y_batch.to('cpu').detach().numpy(),\n",
        "                y_predicted.to('cpu').detach().numpy().argmax(axis=1)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if i % eval_every == 0:\n",
        "            train_loss_history.append(np.mean(local_train_loss_history))\n",
        "            train_acc_history.append(np.mean(local_train_acc_history))\n",
        "            local_train_loss_history, local_train_acc_history = [], []\n",
        "\n",
        "            predictions_val = model(X_val_torch)\n",
        "            val_loss_history.append(loss_function(predictions_val, y_val_torch).to('cpu').detach().item())\n",
        "\n",
        "            acc_score_val = accuracy_score(y_val_torch.cpu().numpy(), predictions_val.to('cpu').detach().numpy().argmax(axis=1))\n",
        "            val_acc_history.append(acc_score_val)\n",
        "            lr_scheduler.step(train_loss_history[-1])\n",
        "\n",
        "            if show_plots:\n",
        "                display.clear_output(wait=True)\n",
        "                plot_train_process(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmAx7gt8j7lo"
      },
      "source": [
        "def full_cycle(my_k, x_train, x_test):\n",
        "  my_X_train_torch = torch.tensor(x_train[:,:my_k], dtype=torch.float32)\n",
        "  my_X_test_torch = torch.tensor(x_test[:,:my_k], dtype=torch.float32)\n",
        "\n",
        "  my_model, my_opt = create_model(my_k)\n",
        "  my_lr_scheduler = ReduceLROnPlateau(my_opt, patience=5)\n",
        "\n",
        "  train_model(my_model, my_opt, my_lr_scheduler, my_X_train_torch, y_train_torch, my_X_test_torch, y_test_torch, show_plots=False)\n",
        "  my_auc_train = roc_auc_score(y_train, my_model(my_X_train_torch).detach().cpu().numpy()[:,1])\n",
        "  my_auc_test = roc_auc_score(y_test, my_model(my_X_test_torch).detach().cpu().numpy()[:,1])\n",
        "  return (my_auc_train,my_auc_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_-RD6Q6kFMA"
      },
      "source": [
        "def plot_dynamics(results, ks):\n",
        "  plt.figure(figsize=[15,10])\n",
        "  plt.ylim((0,1))\n",
        "  plt.plot([mr[0] for mr in results], label='Train roc-auc')\n",
        "  plt.plot([mr[1] for mr in results], label='Test roc-auc')\n",
        "  plt.xticks(range(len(ks)), ks)\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpbR9ZKYkGx4"
      },
      "source": [
        "# Calculate TF\n",
        "def twit_tf(twit):\n",
        "  twit_words = twit.split()\n",
        "  total_words = len(twit_words)\n",
        "  twit_words_tf = dict()\n",
        "  for word in twit_words:\n",
        "    twit_words_tf[word] = twit_words_tf.get(word, 0) + 1/total_words\n",
        "  return twit_words_tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0w6NoqwkILI"
      },
      "source": [
        "# Calculate TF-IDF\n",
        "def get_tf_idf(twit_tfs):\n",
        "  twit_tf_idf = dict()\n",
        "  for word in twit_tfs.keys():\n",
        "    twit_tf_idf[word] = twit_tfs[word]*words_idfs.get(word, 0)\n",
        "  return twit_tf_idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhSc_O08kJX5"
      },
      "source": [
        "# Make vectorization\n",
        "def vectorise(twit_tf_idf):\n",
        "  result = np.zeros(len(words))\n",
        "  for word in twit_tf_idf.keys():\n",
        "    for i in range(len(words)):\n",
        "      if word==words[i]:\n",
        "        result[i] = twit_tf_idf[word]\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmTHNdAgkMG8"
      },
      "source": [
        "# Choose LR\n",
        "def full_cycle_lr(my_lr, x_train, x_test):\n",
        "  my_X_train_torch = torch.tensor(x_train[:,:1000], dtype=torch.float32)\n",
        "  my_X_test_torch = torch.tensor(x_test[:,:1000], dtype=torch.float32)\n",
        "\n",
        "  my_model, my_opt  = create_model(1000)\n",
        "  my_lr_scheduler = ReduceLROnPlateau(my_opt, patience=5)\n",
        "\n",
        "  train_model(my_model, my_opt, my_lr_scheduler, my_X_train_torch, y_train_torch, my_X_test_torch, y_test_torch, show_plots=False)\n",
        "  my_auc_train = roc_auc_score(y_train, my_model(my_X_train_torch).detach().cpu().numpy()[:,1])\n",
        "  my_auc_test = roc_auc_score(y_test, my_model(my_X_test_torch).detach().cpu().numpy()[:,1])\n",
        "  return (my_auc_train,my_auc_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWKafNWQkPyi"
      },
      "source": [
        "def train_nb_clf(X, y):\n",
        "  classes, freq = defaultdict(lambda:0), defaultdict(lambda:0)\n",
        "\n",
        "  for i in range(len(y)):\n",
        "    label = y[i]\n",
        "    classes[label] += 1  \n",
        "    feats = X[i, :]\n",
        "    for feat in feats:\n",
        "      freq[label, feat] += 1\n",
        "\n",
        "  for label, feat in freq:               \n",
        "        freq[label, feat] /= classes[label]\n",
        "  for c in classes:                       \n",
        "        classes[c] /= len(y)\n",
        "\n",
        "  return classes, freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLc6OuiZkUny"
      },
      "source": [
        "def classify(classifier, feats):\n",
        "    classes, prob = classifier\n",
        "    return min(classes.keys(),         \n",
        "        key = lambda cl: -log(classes[cl]) + \\\n",
        "            sum(-log(prob.get((cl,feat), 10**(-7))) for feat in feats))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ukc49Vxkdn7"
      },
      "source": [
        "pretrained_w2v = api.load('glove-twitter-25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUb_TDYUke9F"
      },
      "source": [
        "# Word to vec\n",
        "def twit_to_vec(twit):\n",
        "  twit_vec = []\n",
        "  for word in twit:\n",
        "    try:\n",
        "      word_vec = pretrained_w2v.wv.get_vector(word)\n",
        "      twit_vec.append(word_vec)\n",
        "    except:\n",
        "      pass\n",
        "  if len(twit_vec) == 0:\n",
        "    return np.zeros(25)\n",
        "  else:\n",
        "    return np.mean(twit_vec, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}