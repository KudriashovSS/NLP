# -*- coding: utf-8 -*-
"""Lab01_Poetry_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/neychev/made_nlp_course/blob/spring2021/homeworks/Lab01_Poetry_generation/Lab01_Poetry_generation.ipynb

## Lab 01. Poetry generation

Let's try to generate some poetry using RNNs. 

You have several choices here: 

* The Shakespeare sonnets, file `sonnets.txt` available in the notebook directory.

* Роман в стихах "Евгений Онегин" Александра Сергеевича Пушкина. В предобработанном виде доступен по [ссылке](https://github.com/attatrol/data_sources/blob/master/onegin.txt).

* Some other text source, if it will be approved by the course staff.

Text generation can be designed in several steps:
    
1. Data loading.
2. Dictionary generation.
3. Data preprocessing.
4. Model (neural network) training.
5. Text generation (model evaluation).
"""

import string
import os

"""### Data loading: Shakespeare

Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`). Simple preprocessing is already done for you in the next cell: all technical info is dropped.
"""

if not os.path.exists('sonnets.txt'):
    !wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/homeworks_basic/Lab2_DL/sonnets.txt

with open('sonnets.txt', 'r') as iofile:
    text = iofile.readlines()
    
TEXT_START = 45
TEXT_END = -368
text = text[TEXT_START : TEXT_END]
assert len(text) == 2616

"""In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.

Now variable `text` is a list of strings. Join all the strings into one and lowercase it.
"""

# Join all the strings into one and lowercase it
# Put result into variable text.

# Your great code here
old_text = text
text = ''.join(old_text).lower()

assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'
assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'
print('OK!')

"""Put all the characters, that you've seen in the text, into variable `tokens`."""

tokens = sorted(set(text))
num_tokens = len(tokens)

"""Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"""

# dict <index>:<char>
# Your great code here
token_to_idx = {key:i for i, key in enumerate(tokens)}
# dict <char>:<index>
# Your great code here
idx_to_token = {i:key for key, i in token_to_idx.items()}

import numpy as np
def to_matrix(sentences, max_len=None, pad=token_to_idx[' '], dtype='int32', batch_first = True):
    """Casts a list of sentences into rnn-digestable matrix"""
    
    max_len = max_len or max(map(len, sentences))
    sentences_ix = np.zeros([len(sentences), max_len], dtype) + pad

    for i in range(len(sentences)):
        line_ix = [token_to_idx[c] for c in sentences[i][:max_len]]
        sentences_ix[i, :len(line_ix)] = line_ix
        
    if not batch_first: # convert [batch, time] into [time, batch]
        sentences_ix = np.transpose(sentences_ix)

    return sentences_ix

def sample_texts(sample_size=10):
  sampled_texts = [t.strip()+'\n' for t in text.split('\n') if len(t)>10]
  sample = np.random.choice(sampled_texts, sample_size)
  return sample

to_matrix(sample_texts(32), False)

"""### Building the model

Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.

Let's use vanilla RNN, similar to the one created during the lesson.
"""

import torch, torch.nn as nn
import torch.nn.functional as F

class CharRNNCell(nn.Module):
    """
    Implement the scheme above as torch module
    """
    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):
        super(self.__class__,self).__init__()
        self.num_units = rnn_num_units
        
        self.embedding = nn.Embedding(num_tokens, embedding_size)
        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)
        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)
        
    def forward(self, x, h_prev):
        """
        This method computes h_next(x, h_prev) and log P(x_next | h_next)
        We'll call it repeatedly to produce the whole sequence.
        
        :param x: batch of character ids, containing vector of int64
        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32
        """
        # get vector embedding of x
        x_emb = self.embedding(x)
        
        # compute next hidden state using self.rnn_update
        # hint: use torch.cat(..., dim=...) for concatenation
        x_and_h =  torch.cat([x_emb, h_prev], dim=1)

        h_next = self.rnn_update(x_and_h)
        
        h_next = torch.tanh(h_next)
        
        assert h_next.size() == h_prev.size()
        
        #compute logits for next character probs
        logits = self.rnn_to_logits(h_next)
        
        
        return h_next, F.log_softmax(logits, -1)
    
    def initial_state(self, batch_size):
        """ return rnn state before it processes first input (aka h0) """
        return torch.zeros(batch_size, self.num_units, requires_grad=True)

def rnn_loop(char_rnn, batch_ix):
  batch_size, max_length = batch_ix.size()
  hid_state = char_rnn.initial_state(batch_size)
  logprobs = []

  for x_t in batch_ix.transpose(0,1):
    hid_state, logp_next = char_rnn(x_t, hid_state)
    logprobs.append(logp_next)
  
  return torch.stack(logprobs, dim=1)

char_rnn = CharRNNCell()
criterion = nn.NLLLoss()

batch_ix = to_matrix([text[:50], text[100:200]])
batch_ix = torch.tensor(batch_ix, dtype=torch.int64)
logp_seq = rnn_loop(char_rnn, batch_ix)
assert torch.max(logp_seq).data.numpy() <= 0
assert tuple(logp_seq.size()) ==  batch_ix.shape + (num_tokens,)

predictions_logp = logp_seq[:, :-1]
actual_next_tokens = batch_ix[:, 1:]

loss = criterion(predictions_logp.contiguous().view(-1, num_tokens), actual_next_tokens.contiguous().view(-1))
loss.backward()

for w in char_rnn.parameters():
    assert w.grad is not None and torch.max(torch.abs(w.grad)).data.numpy() != 0, \
        "Loss is not differentiable w.r.t. a weight with shape %s. Check forward method." % (w.size(),)

loss

predictions_logp.shape

actual_next_tokens.shape

from IPython.display import clear_output
import matplotlib.pyplot as plt
from random import sample

char_rnn = CharRNNCell()
criterion = nn.NLLLoss()
opt = torch.optim.Adam(char_rnn.parameters())
history = []

MAX_LENGTH = 50

for i in range(1000):
    batch_ix = to_matrix(sample_texts(20), max_len=MAX_LENGTH)
    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)
    
    logp_seq = rnn_loop(char_rnn, batch_ix)
    
    # compute loss
    predictions_logp = logp_seq[:, :-1]
    actual_next_tokens = batch_ix[:, 1:]

    loss = criterion(
          predictions_logp.contiguous().view(-1, num_tokens), 
          actual_next_tokens.contiguous().view(-1))
    
    # train with backprop
    loss.backward()
    opt.step()
    opt.zero_grad()

    
    history.append(loss.data.numpy())
    if (i+1)%100==0:
        clear_output(True)
        plt.plot(history,label='loss')
        plt.legend()
        plt.show()

assert np.mean(history[:10]) > np.mean(history[-10:]), "RNN didn't converge."

"""Plot the loss function (axis X: number of epochs, axis Y: loss function)."""

def generate_sample(char_rnn, seed_phrase=' Hello', max_length=MAX_LENGTH, temperature=1.0):
    '''
    The function generates text given a phrase of length at least SEQ_LENGTH.
    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase
    :param max_length: maximum output length, including seed_phrase
    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs, 
        smaller temperature converges to the single most likely output.
        
    Be careful with the model output. This model waits logits (not probabilities/log-probabilities)
    of the next symbol.
    '''
    
    x_sequence = [token_to_idx[token] for token in seed_phrase]
    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)
    hid_state = char_rnn.initial_state(batch_size=1)
    
    
    #feed the seed phrase, if any
    for i in range(len(seed_phrase) - 1):
        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)
    
    #start generating
    for _ in range(max_length - len(seed_phrase)):
        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)
        # Be really careful here with the model output
        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]
        
        # sample next token and push it back into x_sequence
        next_ix = np.random.choice(num_tokens, p=p_next)
        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)
        x_sequence = torch.cat([x_sequence, next_ix], dim=1)
        
    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])

generate_sample(char_rnn, seed_phrase=' my', max_length=50, temperature=0.8)

for i in range(10):
  print(generate_sample(char_rnn, seed_phrase=' ', max_length=50, temperature=0.5))

"""### More poetic model

Let's use LSTM instead of vanilla RNN and compare the results.
"""

def sample_text_for_lstm(sample_size, num_rows=4):
  sampled_texts = [t.strip()+'\n' for t in text.split('\n') if len(t)>10]
  starts = map(int, np.random.sample(sample_size)*len(sampled_texts))
  result = [''.join(sampled_texts[s:s+num_rows]) for s in starts]
  return result

class LSMTModel(nn.Module):
  def __init__(self, vocab_size=num_tokens, emb_size=16, hidden_size = 64):
    super(self.__class__,self).__init__()
    self.hidden_size = hidden_size


    self.emb_layer = nn.Embedding(vocab_size, emb_size)
    self.lstm_layer = nn.LSTMCell(emb_size, hidden_size)
    self.to_vocab_layer = nn.Linear(hidden_size, vocab_size)

  def forward(self, x, h_prev, c_prev):
    emb_x = self.emb_layer(x)
    h_n, c_n = self.lstm_layer(emb_x, (h_prev, c_prev))
    lstm_to_vocab = self.to_vocab_layer(h_n)

    return F.log_softmax(lstm_to_vocab, -1), h_n, c_n

  def initial_state(self, batch_size=1):
    h_0 = torch.zeros((batch_size, self.hidden_size))
    c_0 = torch.zeros((batch_size, self.hidden_size))
    return h_0, c_0

def lstm_loop(char_lstm, batch_ix):
  batch_size, max_length = batch_ix.size()
  h_prev, c_prev = char_lstm.initial_state(batch_size)
  logprobs = []

  for x_t in batch_ix.transpose(0,1):
    logp_seq, h_prev, c_prev = char_lstm(x_t, h_prev, c_prev)
    logprobs.append(logp_seq)
  
  return torch.stack(logprobs, dim=1)

lstm_model = LSMTModel(vocab_size=38, emb_size=16)

batch_ix = torch.tensor(to_matrix(sample_text_for_lstm(5)), dtype = torch.long)

logp_seq = lstm_loop(lstm_model, batch_ix)

predictions = logp_seq[:, :-1]
actual_next_tokens = batch_ix[:, 1:]

loss = criterion(predictions.contiguous().view(-1, num_tokens), actual_next_tokens.contiguous().view(-1))
loss.backward()

lstm_model = LSMTModel(vocab_size=38, emb_size=16)
criterion = nn.NLLLoss()
lstm_opt = torch.optim.Adam(lstm_model.parameters())
lstm_history = []

h_prev, c_prev = lstm_model.initial_state()

for i in range(1000):
    batch_ix = torch.tensor(to_matrix(sample_text_for_lstm(20, num_rows=2)), dtype = torch.long)
    
    logp_seq = lstm_loop(lstm_model, batch_ix)
    
    # compute loss
    predictions_logp = logp_seq[:, :-1]
    actual_next_tokens = batch_ix[:, 1:]

    loss = criterion(
          predictions_logp.contiguous().view(-1, num_tokens), 
          actual_next_tokens.contiguous().view(-1))
    
    # train with backprop
    loss.backward()
    lstm_opt.step()
    lstm_opt.zero_grad()

    
    lstm_history.append(loss.data.numpy())
    if (i+1)%100==0:
        clear_output(True)
        plt.plot(lstm_history,label='loss')
        plt.legend()
        plt.show()

assert np.mean(lstm_history[:10]) > np.mean(lstm_history[-10:]), "RNN didn't converge."

"""Plot the loss function of the number of epochs. Does the final loss become better?

LSTM trains not so good. RNN showed much better results

Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.

Evaluate the results visually, try to interpret them.
"""

def generate_sample_lstm(char_lstm, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):   
    x_sequence = [token_to_idx[token] for token in seed_phrase]
    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)
    h_prev, c_prev = char_lstm.initial_state(batch_size=1)
    
    #feed the seed phrase, if any
    for i in range(len(seed_phrase) - 1):
        _, h_prev, c_prev = char_lstm(x_sequence[:, i], h_prev, c_prev)
    
    #start generating
    for _ in range(max_length - len(seed_phrase)):
        logp_next, h_prev, c_prev = char_lstm(x_sequence[:, -1], h_prev, c_prev)
        # Be really careful here with the model output
        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]
        
        # sample next token and push it back into x_sequence
        next_ix = np.random.choice(num_tokens, p=p_next)
        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)
        x_sequence = torch.cat([x_sequence, next_ix], dim=1)
        x_sequence_np = np.squeeze(x_sequence.data.numpy())
  
    return ''.join([tokens[ix] for ix in x_sequence_np])

for t in [0.1, 0.2, 0.5, 1.0, 2.0]:
  print('t = ', t)
  for _ in range(4):
    print(generate_sample_lstm(lstm_model, 'sometimes', temperature=t, max_length=50))
  print('--------------------------------')

"""Quality of LSTM predictiions seems to be  depends a lot on start phrase. Overall LSTM seems to be more connected. And big dependency on temperature is observed.

### Saving and loading models

Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html]).
"""

# Saving and loading code here
def save_model(model, path):
  torch.save(model.state_dict(), path)

def load_model(model_class, path):
  model_loaded = model_class()
  model_loaded.load_state_dict(torch.load(path))
  model_loaded.eval()
  return model_loaded

lstm_path = 'lstm_model.t'
save_model(lstm_model, lstm_path)
lstm_loaded_model = load_model(LSMTModel, lstm_path)
generate_sample_lstm(lstm_loaded_model, ' ')

rnn_path = 'rnn_model.t'
save_model(char_rnn, rnn_path)
rnn_loaded_model = load_model(CharRNNCell, rnn_path)
generate_sample(rnn_loaded_model, ' ', temperature=0.2)

"""### References
1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> 
There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.
2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>
3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)
"""